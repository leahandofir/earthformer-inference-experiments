{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100",
   "authorship_tag": "ABX9TyNmWWp+9+oIu4Z8kc2Enl5a",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "gpuClass": "standard",
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/leahandofir/earthformer-inference-experiments/blob/main/earthformer_SEVIR_train_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Trying to run the training code on a small dataset (https://github.com/amazon-science/earth-forecasting-transformer/tree/main/scripts/cuboid_transformer/sevir)"
   ],
   "metadata": {
    "id": "RVAMqqlGcy-2",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Install dependecies"
   ],
   "metadata": {
    "id": "zFPXvZfWcy9A",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHRoFQNocmqI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!curl 'https://raw.githubusercontent.com/leahandofir/earthformer-inference-experiments/main/prerequisites.sh' -o ./prerequisites.sh\n",
    "!bash ./prerequisites.sh\n",
    "# check cuda version\n",
    "!nvcc --version\n",
    "%env HOME=/content/earth-forecasting-transformer\n",
    "%cd ~"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# download all 2019 data\n",
    "!aws s3 cp --no-sign-request --recursive --region us-west-2 s3://sevir/data/vil/2019 ~/datasets/sevir/data/vil/2019\n",
    "!aws s3 cp --no-sign-request --region us-west-2 s3://sevir/CATALOG.csv ~/datasets/sevir/CATALOG.csv"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f-sWS46ZiUB-",
    "outputId": "07dcae92-4436-4b4d-c929-7ac03b27a14b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "download: s3://sevir/data/vil/2019/SEVIR_VIL_STORMEVENTS_2019_0701_1231.h5 to datasets/sevir/data/vil/2019/SEVIR_VIL_STORMEVENTS_2019_0701_1231.h5\n",
      "download: s3://sevir/data/vil/2019/SEVIR_VIL_STORMEVENTS_2019_0101_0630.h5 to datasets/sevir/data/vil/2019/SEVIR_VIL_STORMEVENTS_2019_0101_0630.h5\n",
      "download: s3://sevir/data/vil/2019/SEVIR_VIL_RANDOMEVENTS_2019_0901_1231.h5 to datasets/sevir/data/vil/2019/SEVIR_VIL_RANDOMEVENTS_2019_0901_1231.h5\n",
      "download: s3://sevir/data/vil/2019/SEVIR_VIL_RANDOMEVENTS_2019_0501_0831.h5 to datasets/sevir/data/vil/2019/SEVIR_VIL_RANDOMEVENTS_2019_0501_0831.h5\n",
      "download: s3://sevir/data/vil/2019/SEVIR_VIL_RANDOMEVENTS_2019_0101_0430.h5 to datasets/sevir/data/vil/2019/SEVIR_VIL_RANDOMEVENTS_2019_0101_0430.h5\n",
      "download: s3://sevir/CATALOG.csv to datasets/sevir/CATALOG.csv     \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%cd scripts/cuboid_transformer/sevir"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "akSlaUUve5Eq",
    "outputId": "8145e80c-57af-4602-ab23-5c1082c63853",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/earth-forecasting-transformer/scripts/cuboid_transformer/sevir\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from omegaconf import OmegaConf\n",
    "conf = OmegaConf.load('./cfg_sevir.yaml')\n",
    "conf.dataset.start_date = [2019, 1, 1]\n",
    "conf.dataset.end_date = [2019, 12, 1]\n",
    "conf.dataset.train_val_split_date = [2019, 1, 31]\n",
    "conf.dataset.train_test_split_date = [2019, 5, 1]\n",
    "conf.optim.micro_batch_size = 1\n",
    "OmegaConf.save(config=conf, f='./my_cfg_sevir.yaml')"
   ],
   "metadata": {
    "id": "7lhRUXs6fqAi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!MASTER_ADDR=localhost MASTER_PORT=10001 python train_cuboid_sevir.py --gpus 1 --cfg my_cfg_sevir.yaml --ckpt_name last.ckpt --save tmp_sevir"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sjlof6eQfp5f",
    "outputId": "eefba805-8e63-435c-9bb7-8239cb24b2f1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Global seed set to 0\n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 0\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory /content/earth-forecasting-transformer/scripts/cuboid_transformer/sevir/experiments/tmp_sevir/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "Restoring states from the checkpoint path at /content/earth-forecasting-transformer/scripts/cuboid_transformer/sevir/experiments/tmp_sevir/checkpoints/last.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.parallel.DistributedDataParallel is deprecated and will be removed by the end of February 2023.\n",
      "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
      "Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.\n",
      "\n",
      "  | Name            | Type                   | Params\n",
      "-----------------------------------------------------------\n",
      "0 | torch_nn_module | CuboidTransformerModel | 8.7 M \n",
      "1 | valid_mse       | MeanSquaredError       | 0     \n",
      "2 | valid_mae       | MeanAbsoluteError      | 0     \n",
      "3 | valid_score     | SEVIRSkillScore        | 0     \n",
      "4 | test_mse        | MeanSquaredError       | 0     \n",
      "5 | test_mae        | MeanAbsoluteError      | 0     \n",
      "6 | test_score      | SEVIRSkillScore        | 0     \n",
      "-----------------------------------------------------------\n",
      "8.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.7 M     Total params\n",
      "34.639    Total estimated model params size (MB)\n",
      "2023-05-14 10:50:41.687690: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Restored all states from the checkpoint file at /content/earth-forecasting-transformer/scripts/cuboid_transformer/sevir/experiments/tmp_sevir/checkpoints/last.ckpt\n",
      "Sanity Checking DataLoader 0: 100% 2/2 [00:02<00:00,  1.29s/it]/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_frame_mse_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_frame_mae_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_loss_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_csi_16_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_csi_74_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_csi_133_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_csi_160_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_csi_181_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_csi_219_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_csi_avg_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_pod_16_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_pod_74_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_pod_133_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_pod_160_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_pod_181_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_pod_219_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_pod_avg_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_sucr_16_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_sucr_74_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_sucr_133_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_sucr_160_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_sucr_181_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_sucr_219_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_sucr_avg_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_bias_16_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_bias_74_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_bias_133_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_bias_160_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_bias_181_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_bias_219_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('valid_bias_avg_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Global seed set to 0\n",
      "Restoring states from the checkpoint path at /content/earth-forecasting-transformer/scripts/cuboid_transformer/sevir/experiments/tmp_sevir/checkpoints/model-epoch=001.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at /content/earth-forecasting-transformer/scripts/cuboid_transformer/sevir/experiments/tmp_sevir/checkpoints/model-epoch=001.ckpt\n",
      "Testing DataLoader 0: 100% 7110/7110 [13:56<00:00,  8.50it/s]/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_frame_mse_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_frame_mae_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_csi_16_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_csi_74_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_csi_133_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_csi_160_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_csi_181_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_csi_219_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_csi_avg_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_pod_16_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_pod_74_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_pod_133_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_pod_160_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_pod_181_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_pod_219_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_pod_avg_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_sucr_16_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_sucr_74_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_sucr_133_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_sucr_160_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_sucr_181_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_sucr_219_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_sucr_avg_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_bias_16_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_bias_74_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_bias_133_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_bias_160_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_bias_181_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_bias_219_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('test_bias_avg_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "Testing DataLoader 0: 100% 7110/7110 [13:56<00:00,  8.50it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_bias_133_epoch   \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m           0.0           \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_bias_160_epoch   \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m           0.0           \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_bias_16_epoch    \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m    3.488028049468994    \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_bias_181_epoch   \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m           0.0           \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_bias_219_epoch   \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m           0.0           \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_bias_74_epoch    \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m    1.581799030303955    \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_bias_avg_epoch   \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.8449711799621582    \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_csi_133_epoch    \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m           0.0           \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_csi_160_epoch    \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m           0.0           \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m    test_csi_16_epoch    \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.5560194253921509    \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_csi_181_epoch    \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m           0.0           \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_csi_219_epoch    \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m           0.0           \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m    test_csi_74_epoch    \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.42763227224349976   \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_csi_avg_epoch    \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.16394194960594177   \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m  test_frame_mae_epoch   \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.05607447028160095   \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m  test_frame_mse_epoch   \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m  0.011033964343369007   \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_pod_133_epoch    \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m           0.0           \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_pod_160_epoch    \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m           0.0           \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m    test_pod_16_epoch    \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.8199182748794556    \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_pod_181_epoch    \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m           0.0           \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_pod_219_epoch    \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m           0.0           \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m    test_pod_74_epoch    \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.5606685876846313    \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_pod_avg_epoch    \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.23009781539440155   \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_sucr_133_epoch   \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m           0.0           \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_sucr_160_epoch   \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m           0.0           \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_sucr_16_epoch    \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.6333665251731873    \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_sucr_181_epoch   \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m           0.0           \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_sucr_219_epoch   \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m           0.0           \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_sucr_74_epoch    \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.6431394219398499    \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m   test_sucr_avg_epoch   \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.21275098621845245   \u001B[0m\u001B[35m \u001B[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!zip /content/earth-forecasting-transformer/scripts/cuboid_transformer/sevir/experiments/tmp_sevir.zip /content/earth-forecasting-transformer/scripts/cuboid_transformer/sevir/experiments/tmp_sevir"
   ],
   "metadata": {
    "id": "2XBSGvKimi9H",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "330bfe6c-1a27-4edf-d341-4a2b898f7d91",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  adding: content/earth-forecasting-transformer/scripts/cuboid_transformer/sevir/experiments/tmp_sevir/ (stored 0%)\n"
     ]
    }
   ]
  }
 ]
}