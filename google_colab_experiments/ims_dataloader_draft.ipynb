{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYulGa780YL9JDyPXK63TH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC-OPRo53EVY"
      },
      "outputs": [],
      "source": [
        "\n",
        "!curl 'https://raw.githubusercontent.com/leahandofir/earthformer-inference-experiments/main/google_colab_experiments/prerequisites.sh' -o ./prerequisites.sh\n",
        "!bash ./prerequisites.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Code is adapted from https://github.com/MIT-AI-Accelerator/neurips-2020-sevir. Their license is MIT License.\"\"\"\n",
        "\n",
        "import os\n",
        "from typing import List, Union, Dict, Sequence\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "import numpy.random as nprand\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import h5py\n",
        "from ...utils.layout import change_layout_np, change_layout_torch\n",
        "import torch\n",
        "from torch.nn.functional import avg_pool2d\n",
        "from ...config import cfg\n",
        "\n",
        "\n",
        "# IMS dataset constants\n",
        "IMS_DATA_TYPES        = {\"MIDDLE_EAST_VIS\", \"MIDDLE_EAST_DAY_CLOUDS\", \"MIDDLE_EAST_COLORED\", \"MIDDLE_EAST_IR\"}\n",
        "IMS_RAW_DTYPES        = {'MIDDLE_EAST_VIS': np.uint8} # @ currently only VIS raw-type is known\n",
        "# IMS_DATA_SHAPE = {'MIDDLE_EAST_VIS': (48, 48), }    # @ to be decided later\n",
        "PREPROCESS_SCALE_IMS  = {'MIDDLE_EAST_VIS': 1/255}\n",
        "PREPROCESS_OFFSET_IMS = {'MIDDLE_EAST_VIS': 0}\n",
        "                         \n",
        "VALID_LAYOUTS     = {'NHWT', 'NTHW', 'NTCHW', 'NTHWC', 'TNHW', 'TNCHW'}\n",
        "VALID_SPLIT_MODES = {'ceil', 'floor', 'uneven'} \n",
        "\n",
        "# IMS dataset directory\n",
        "IMS_ROOT_DIR = os.path.join(cfg.datasets_dir, \"ims\")\n",
        "IMS_CATALOG = os.path.join(IMS_ROOT_DIR, \"CATALOG.csv\")\n",
        "IMS_DATA_DIR = os.path.join(IMS_ROOT_DIR, \"data\")\n",
        "IMS_RAW_SEQ_LEN = 288\n",
        "\n",
        "\n",
        "class IMSDataLoader:\n",
        "    r\"\"\"\n",
        "    DataLoader that loads IMS sequences, and spilts each event\n",
        "    into segments according to specified sequence length.\n",
        "\n",
        "    Event Frames:\n",
        "        [-----------------------raw_seq_len----------------------]\n",
        "        [-----seq_len-----]\n",
        "        <--stride-->[-----seq_len-----]\n",
        "                    <--stride-->[-----seq_len-----]\n",
        "                                        ...\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 data_type: str = 'MIDDLE_EAST_VIS',\n",
        "                 seq_len: int = 49,\n",
        "                 raw_seq_len: int = 288,\n",
        "                 sample_mode: str = 'sequent',\n",
        "                 stride: int = 12,\n",
        "                 batch_size: int = 1,\n",
        "                 layout: str = 'NHWT',\n",
        "                 num_shard: int = 1,    \n",
        "                 rank: int = 0,\n",
        "                 split_mode: str = \"uneven\",\n",
        "                 ims_catalog: Union[str, pd.DataFrame] = None,\n",
        "                 ims_data_dir: str = None,\n",
        "                 start_date: datetime.datetime = None,\n",
        "                 end_date: datetime.datetime = None,\n",
        "                 datetime_filter=None,\n",
        "                 catalog_filter='default',\n",
        "                 shuffle: bool = False,\n",
        "                 shuffle_seed: int = 1,\n",
        "                 output_type=np.float32,\n",
        "                 preprocess: bool = True,\n",
        "                 rescale_method: str = 'ims',\n",
        "                 downsample_dict: Dict[str, Sequence[int]] = None,\n",
        "                 verbose: bool = False):\n",
        "        r\"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        data_types\n",
        "            A subset of SEVIR_DATA_TYPES.\n",
        "        seq_len\n",
        "            The length of the data sequences. Should be smaller than the max length raw_seq_len.\n",
        "        raw_seq_len\n",
        "            The length of the raw data sequences.\n",
        "        sample_mode\n",
        "            'random' or 'sequent'\n",
        "        stride\n",
        "            Useful when sample_mode == 'sequent'\n",
        "            stride must not be smaller than out_len to prevent data leakage in testing.\n",
        "        batch_size\n",
        "            Number of sequences in one batch.\n",
        "        layout\n",
        "            str: consists of batch_size 'N', seq_len 'T', channel 'C', height 'H', width 'W'\n",
        "            The layout of sampled data. Raw data layout is 'NHWT'.\n",
        "            valid layout: 'NHWT', 'NTHW', 'NTCHW', 'TNHW', 'TNCHW'.\n",
        "        num_shard\n",
        "            Split the whole dataset into num_shard parts for distributed training.\n",
        "        rank\n",
        "            Rank of the current process within num_shard.\n",
        "        split_mode: str\n",
        "            if 'ceil', all `num_shard` dataloaders have the same length = ceil(total_len / num_shard).\n",
        "            Different dataloaders may have some duplicated data batches, if the total size of datasets is not divided by num_shard.\n",
        "            if 'floor', all `num_shard` dataloaders have the same length = floor(total_len / num_shard).\n",
        "            The last several data batches may be wasted, if the total size of datasets is not divided by num_shard.\n",
        "            if 'uneven', the last datasets has larger length when the total length is not divided by num_shard.\n",
        "            The uneven split leads to synchronization error in dist.all_reduce() or dist.barrier().\n",
        "            See related issue: https://github.com/pytorch/pytorch/issues/33148\n",
        "            Notice: this also affects the behavior of `self.use_up`.\n",
        "        sevir_catalog\n",
        "            Name of SEVIR catalog CSV file.\n",
        "        sevir_data_dir\n",
        "            Directory path to SEVIR data.\n",
        "        start_date\n",
        "            Start time of SEVIR samples to generate.\n",
        "        end_date\n",
        "            End time of SEVIR samples to generate.\n",
        "        datetime_filter\n",
        "            function\n",
        "            Mask function applied to time_utc column of catalog (return true to keep the row).\n",
        "            Pass function of the form   lambda t : COND(t)\n",
        "            Example:  lambda t: np.logical_and(t.dt.hour>=13,t.dt.hour<=21)  # Generate only day-time events\n",
        "        catalog_filter\n",
        "            function or None or 'default'\n",
        "            Mask function applied to entire catalog dataframe (return true to keep row).\n",
        "            Pass function of the form lambda catalog:  COND(catalog)\n",
        "            Example:  lambda c:  [s[0]=='S' for s in c.id]   # Generate only the 'S' events\n",
        "        shuffle\n",
        "            bool, If True, data samples are shuffled before each epoch.\n",
        "        shuffle_seed\n",
        "            int, Seed to use for shuffling.\n",
        "        output_type\n",
        "            np.dtype, dtype of generated tensors\n",
        "        preprocess\n",
        "            bool, If True, self.preprocess_data_dict(data_dict) is called before each sample generated\n",
        "        downsample_dict:\n",
        "            dict, downsample_dict.keys() == data_types. downsample_dict[key] is a Sequence of (t_factor, h_factor, w_factor),\n",
        "            representing the downsampling factors of all dimensions.\n",
        "        verbose\n",
        "            bool, verbose when opening raw data files\n",
        "        \"\"\"\n",
        "        super(IMSDataLoader, self).__init__()\n",
        "\n",
        "        # setting to default values\n",
        "        if ims_catalog is None:\n",
        "            ims_catalog = IMS_CATALOG\n",
        "        if ims_data_dir is None:\n",
        "            ims_data_dir = IMS_DATA_DIR\n",
        "\n",
        "        assert data_type in IMS_DATA_TYPES\n",
        "        self.data_type = data_type\n",
        "\n",
        "        # configs which should not be modified\n",
        "        self._dtypes = IMS_RAW_DTYPES\n",
        "        self.data_shape = IMS_DATA_SHAPE\n",
        "\n",
        "        self.raw_seq_len = raw_seq_len\n",
        "        assert seq_len <= self.raw_seq_len, f'seq_len must not be larger than raw_seq_len = {raw_seq_len}, got {seq_len}.'\n",
        "        self.seq_len = seq_len\n",
        "        assert sample_mode in ['random', 'sequent'], f'Invalid sample_mode = {sample_mode}, must be \\'random\\' or \\'sequent\\'.'\n",
        "        self.sample_mode = sample_mode # @ check this out later\n",
        "        self.stride = stride           # @ check this out later\n",
        "        self.batch_size = batch_size\n",
        "        if layout not in VALID_LAYOUTS:\n",
        "            raise ValueError(f'Invalid layout = {layout}! Must be one of {VALID_LAYOUTS}.')\n",
        "        self.layout = layout\n",
        "        self.num_shard = num_shard   \n",
        "        self.rank = rank              # @ check this out later \n",
        "        if split_mode not in VALID_SPLIT_MODES:\n",
        "            raise ValueError(f'Invalid split_mode: {split_mode}! Must be one of {VALID_SPLIT_MODES}.')\n",
        "        self.split_mode = split_mode  # @ check this out later \n",
        "        self._samples = None          # @ check this out later \n",
        "        self._hdf_files = {}          # @ check this out later \n",
        "        if isinstance(ims_catalog, str):\n",
        "            self.catalog = pd.read_csv(ims_catalog, parse_dates=['time_utc'], low_memory=False)\n",
        "        else:\n",
        "            self.catalog = ims_catalog\n",
        "        self.ims_data_dir = ims_data_dir\n",
        "        self.start_date = start_date\n",
        "        self.end_date = end_date\n",
        "        self.datetime_filter = datetime_filter\n",
        "        self.catalog_filter = catalog_filter\n",
        "        \n",
        "        if self.start_date is not None:\n",
        "            self.catalog = self.catalog[self.catalog.time_utc > self.start_date]\n",
        "        if self.end_date is not None:\n",
        "            self.catalog = self.catalog[self.catalog.time_utc <= self.end_date]\n",
        "        if self.datetime_filter:\n",
        "            self.catalog = self.catalog[self.datetime_filter(self.catalog.time_utc)]        \n",
        "        if self.catalog_filter is not None:\n",
        "            if self.catalog_filter == 'default':\n",
        "                self.catalog_filter = lambda c: c.pct_missing == 0\n",
        "            self.catalog = self.catalog[self.catalog_filter(self.catalog)]\n",
        "\n",
        "        self.shuffle = shuffle                  # @ check this out later \n",
        "        self.shuffle_seed = int(shuffle_seed)   # @ check this out later \n",
        "        self.output_type = output_type          # @ check this out later \n",
        "        self.preprocess = preprocess\n",
        "        self.downsample_dict = downsample_dict  # @ check this out later \n",
        "        self.rescale_method = rescale_method    # @ check this out later \n",
        "        self.verbose = verbose\n",
        "\n",
        "        self._compute_samples()                 \n",
        "        self._open_files(verbose=self.verbose)   \n",
        "        self.reset()                            # @ check this out later \n",
        "\n",
        "    def _compute_samples(self):\n",
        "        \"\"\"\n",
        "        Computes the list of samples in catalog to be used. This sets self._samples\n",
        "        \"\"\"\n",
        "        self._samples = self.catalog[self.catalog.img_type == self.data_type] \n",
        "        if self.shuffle:\n",
        "            self.shuffle_samples()\n",
        "\n",
        "    def shuffle_samples(self):\n",
        "        self._samples = self._samples.sample(frac=1, random_state=self.shuffle_seed)\n",
        "\n",
        "    def _open_files(self, verbose=True):\n",
        "        \"\"\"\n",
        "        Opens HDF files\n",
        "        \"\"\"\n",
        "        file_names = self._samples['file_name'].unique()\n",
        "        for f in file_names:\n",
        "          self._hdf_files[f] = h5py.File(os.path.join(self.ims_data_dir, f), 'r')\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"\n",
        "        Closes all open file handles\n",
        "        \"\"\"\n",
        "        for f in self._hdf_files:\n",
        "            self._hdf_files[f].close()\n",
        "        self._hdf_files = {}\n",
        "\n",
        "    @property\n",
        "    def num_seq_per_event(self):\n",
        "      \"\"\"\n",
        "      How many samples are created from each event\n",
        "      \"\"\"\n",
        "      return 1 + (self.raw_seq_len - self.seq_len) // self.stride\n",
        "\n",
        "    @property\n",
        "    def total_num_seq(self):\n",
        "        \"\"\"\n",
        "        The total number of sequences within each shard.\n",
        "        Notice that it is not the product of `self.num_seq_per_event` and `self.total_num_event`.\n",
        "        \"\"\"\n",
        "        return int(self.num_seq_per_event * self.num_event)\n",
        "\n",
        "    @property\n",
        "    def total_num_event(self):\n",
        "        \"\"\"\n",
        "        The total number of events in the whole dataset, before split into different shards.\n",
        "        \"\"\"\n",
        "        return int(self._samples.shape[0])\n",
        "\n",
        "    @property\n",
        "    def start_event_idx(self):\n",
        "        \"\"\"\n",
        "        The event idx used in certain rank should satisfy event_idx >= start_event_idx\n",
        "        \"\"\"\n",
        "        # @ current guess: this is the start of a specific shard's workload\n",
        "        return self.total_num_event // self.num_shard * self.rank\n",
        "\n",
        "    @property\n",
        "    def end_event_idx(self):\n",
        "        \"\"\"\n",
        "        The event idx used in certain rank should satisfy event_idx < end_event_idx\n",
        "        \"\"\"\n",
        "        if self.split_mode == 'ceil':\n",
        "            _last_start_event_idx = self.total_num_event // self.num_shard * (self.num_shard - 1)\n",
        "            _num_event = self.total_num_event - _last_start_event_idx\n",
        "            return self.start_event_idx + _num_event\n",
        "        if self.split_mode == 'floor':\n",
        "            return self.total_num_event // self.num_shard * (self.rank + 1)\n",
        "        if self.split_mode == 'uneven':\n",
        "            if self.rank == self.num_shard - 1:  # the last process\n",
        "                return self.total_num_event\n",
        "            else:\n",
        "                return self.total_num_event // self.num_shard * (self.rank + 1)\n",
        "\n",
        "    @property\n",
        "    def num_event(self):\n",
        "        \"\"\"\n",
        "        The number of events split into each rank\n",
        "        \"\"\"\n",
        "        return self.end_event_idx - self.start_event_idx\n",
        "\n",
        "    def _read_data(self, row, data):\n",
        "        \"\"\"\n",
        "        Iteratively read data into data dict. Finally data[imgt] gets shape (batch_size, height, width, raw_seq_len).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        row\n",
        "            A series with fields IMGTYPE_filename, IMGTYPE_index, IMGTYPE_time_index.\n",
        "        data\n",
        "            Dict, data[imgt] is a data tensor with shape = (tmp_batch_size, height, width, raw_seq_len).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        data\n",
        "            Updated data. Updated shape = (tmp_batch_size + 1, height, width, raw_seq_len).\n",
        "        \"\"\"\n",
        "        idx = row['file_index']\n",
        "        data_i = self._hdf_files[row['file_name']][self.data_type][idx:idx + 1, :, :, slice(0, None)] # @ need to make sure slicing dimensions are okay\n",
        "        return np.concatenate((data, data_i), axis=0)\n",
        "\n",
        "    \n",
        "    def _old_save_downsampled_dataset(self, save_dir, downsample_dict, verbose=True):\n",
        "        \"\"\"\n",
        "        This method does not save .h5 dataset correctly. There are some batches missed due to unknown error.\n",
        "        E.g., the first converted .h5 file `SEVIR_VIL_RANDOMEVENTS_2017_0501_0831.h5` only has batch_dim = 1414,\n",
        "        while it should be 1440 in the original .h5 file.\n",
        "        \"\"\"\n",
        "        import os\n",
        "        from skimage.measure import block_reduce\n",
        "        assert not os.path.exists(save_dir), f\"save_dir {save_dir} already exists!\"\n",
        "        os.makedirs(save_dir)\n",
        "        sample_counter = 0\n",
        "        for index, row in self._samples.iterrows():\n",
        "            if verbose:\n",
        "                print(f\"Downsampling {sample_counter}-th data item.\", end='\\r')\n",
        "            for data_type in self.data_types:\n",
        "                fname = row[f'{data_type}_filename']\n",
        "                idx = row[f'{data_type}_index']\n",
        "                t_slice = slice(0, None)\n",
        "                if data_type == 'lght':\n",
        "                    lght_data = self._hdf_files[fname][idx][:]\n",
        "                    data_i = self._lght_to_grid(lght_data, t_slice)\n",
        "                else:\n",
        "                    data_i = self._hdf_files[fname][data_type][idx:idx + 1, :, :, t_slice]\n",
        "                # Downsample t\n",
        "                t_slice = [slice(None, None), ] * 4\n",
        "                t_slice[-1] = slice(None, None, downsample_dict[data_type][0])  # layout = 'NHWT'\n",
        "                data_i = data_i[tuple(t_slice)]\n",
        "                # Downsample h, w\n",
        "                data_i = block_reduce(data_i,\n",
        "                                      block_size=(1, *downsample_dict[data_type][1:], 1),\n",
        "                                      func=np.max)\n",
        "                # Save as new .h5 file\n",
        "                new_file_path = os.path.join(save_dir, fname)\n",
        "                if not os.path.exists(new_file_path):\n",
        "                    if not os.path.exists(os.path.dirname(new_file_path)):\n",
        "                        os.makedirs(os.path.dirname(new_file_path))\n",
        "                    # Create dataset\n",
        "                    with h5py.File(new_file_path, 'w') as hf:\n",
        "                        hf.create_dataset(\n",
        "                            data_type, data=data_i,\n",
        "                            maxshape=(None, *data_i.shape[1:]))\n",
        "                else:\n",
        "                    # Append\n",
        "                    with h5py.File(new_file_path, 'a') as hf:\n",
        "                        hf[data_type].resize((hf[data_type].shape[0] + data_i.shape[0]), axis=0)\n",
        "                        hf[data_type][-data_i.shape[0]:] = data_i\n",
        "\n",
        "            sample_counter += 1\n",
        "\n",
        "    def save_downsampled_dataset(self, save_dir, downsample_dict, verbose=True):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        save_dir\n",
        "        downsample_dict:    Dict[Sequence[int]]\n",
        "            Notice that this is different from `self.downsample_dict`, which is used during runtime.\n",
        "        \"\"\"\n",
        "        import os\n",
        "        from skimage.measure import block_reduce\n",
        "        from ...utils.utils import path_splitall\n",
        "        assert not os.path.exists(save_dir), f\"save_dir {save_dir} already exists!\"\n",
        "        os.makedirs(save_dir)\n",
        "        for fname, hdf_file in self._hdf_files.items():\n",
        "            if verbose:\n",
        "                print(f\"Downsampling data in {fname}.\")\n",
        "            data_type = path_splitall(fname)[0]\n",
        "            if data_type == 'lght':\n",
        "                # TODO: how to get idx?\n",
        "                raise NotImplementedError\n",
        "                # lght_data = self._hdf_files[fname][idx][:]\n",
        "                # t_slice = slice(0, None)\n",
        "                # data_i = self._lght_to_grid(lght_data, t_slice)\n",
        "            else:\n",
        "                data_i = self._hdf_files[fname][data_type]\n",
        "            # Downsample t\n",
        "            t_slice = [slice(None, None), ] * 4\n",
        "            t_slice[-1] = slice(None, None, downsample_dict[data_type][0])  # layout = 'NHWT'\n",
        "            data_i = data_i[tuple(t_slice)]\n",
        "            # Downsample h, w\n",
        "            data_i = block_reduce(data_i,\n",
        "                                  block_size=(1, *downsample_dict[data_type][1:], 1),\n",
        "                                  func=np.max)\n",
        "            # Save as new .h5 file\n",
        "            new_file_path = os.path.join(save_dir, fname)\n",
        "            if not os.path.exists(os.path.dirname(new_file_path)):\n",
        "                os.makedirs(os.path.dirname(new_file_path))\n",
        "            # Create dataset\n",
        "            with h5py.File(new_file_path, 'w') as hf:\n",
        "                hf.create_dataset(\n",
        "                    data_type, data=data_i,\n",
        "                    maxshape=(None, *data_i.shape[1:]))\n",
        "\n",
        "    @property\n",
        "    def sample_count(self):\n",
        "        \"\"\"\n",
        "        Record how many times self.__next__() is called.\n",
        "        \"\"\"\n",
        "        return self._sample_count\n",
        "\n",
        "    def inc_sample_count(self):\n",
        "        self._sample_count += 1\n",
        "\n",
        "    @property\n",
        "    def curr_event_idx(self):\n",
        "        return self._curr_event_idx\n",
        "\n",
        "    @property\n",
        "    def curr_seq_idx(self):\n",
        "        \"\"\"\n",
        "        Used only when self.sample_mode == 'sequent'\n",
        "        \"\"\"\n",
        "        return self._curr_seq_idx\n",
        "\n",
        "    def set_curr_event_idx(self, val):\n",
        "        self._curr_event_idx = val\n",
        "\n",
        "    def set_curr_seq_idx(self, val):\n",
        "        \"\"\"\n",
        "        Used only when self.sample_mode == 'sequent'\n",
        "        \"\"\"\n",
        "        self._curr_seq_idx = val\n",
        "\n",
        "    def reset(self, shuffle: bool = None):\n",
        "        self.set_curr_event_idx(val=self.start_event_idx)\n",
        "        self.set_curr_seq_idx(0)\n",
        "        self._sample_count = 0\n",
        "        if shuffle is None:\n",
        "            shuffle = self.shuffle\n",
        "        if shuffle:\n",
        "            self.shuffle_samples()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Used only when self.sample_mode == 'sequent'\n",
        "        \"\"\"\n",
        "        return self.total_num_seq // self.batch_size\n",
        "\n",
        "    @property\n",
        "    def use_up(self):\n",
        "        \"\"\"\n",
        "        Check if dataset is used up in 'sequent' mode.\n",
        "        \"\"\"\n",
        "        if self.sample_mode == 'random':\n",
        "            return False\n",
        "        else:   # self.sample_mode == 'sequent'\n",
        "            # compute the remaining number of sequences in current event\n",
        "            curr_event_remain_seq = self.num_seq_per_event - self.curr_seq_idx\n",
        "            all_remain_seq = curr_event_remain_seq + (\n",
        "                        self.end_event_idx - self.curr_event_idx - 1) * self.num_seq_per_event\n",
        "            if self.split_mode == \"floor\":\n",
        "                # This approach does not cover all available data, but avoid dealing with masks\n",
        "                return all_remain_seq < self.batch_size\n",
        "            else:\n",
        "                return all_remain_seq <= 0\n",
        "\n",
        "    def _load_event_batch(self, event_idx, event_batch_size):\n",
        "        \"\"\"\n",
        "        Loads a selected batch of events (not batch of sequences) into memory.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx\n",
        "        event_batch_size\n",
        "            event_batch[i] = all_type_i_available_events[idx:idx + event_batch_size]\n",
        "        Returns\n",
        "        -------\n",
        "        event_batch\n",
        "            list of event batches.\n",
        "            event_batch[i] is the event batch of the i-th data type.\n",
        "            Each event_batch[i] is a np.ndarray with shape = (event_batch_size, height, width, raw_seq_len)\n",
        "        \"\"\"\n",
        "        event_idx_slice_end = event_idx + event_batch_size\n",
        "        pad_size = 0\n",
        "        if event_idx_slice_end > self.end_event_idx:\n",
        "            pad_size = event_idx_slice_end - self.end_event_idx\n",
        "            event_idx_slice_end = self.end_event_idx\n",
        "        pd_batch = self._samples.iloc[event_idx:event_idx_slice_end]\n",
        "        data = np.empty(0) # @ check what dimensions we want to initialize data with\n",
        "        for index, row in pd_batch.iterrows():\n",
        "            data = self._read_data(row, data)\n",
        "        if pad_size > 0:\n",
        "            event_batch = []\n",
        "            pad_shape = [pad_size, ] + list(data.shape[1:])\n",
        "            data_pad = np.concatenate((data.astype(self.output_type),\n",
        "                                        np.zeros(pad_shape, dtype=self.output_type)),\n",
        "                                      axis=0)\n",
        "            event_batch.append(data_pad)\n",
        "        else:\n",
        "            event_batch = data.astype(self.output_type)\n",
        "        return event_batch\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.sample_mode == 'random':\n",
        "            self.inc_sample_count()\n",
        "            ret_dict = self._random_sample()\n",
        "        else:\n",
        "            if self.use_up:\n",
        "                raise StopIteration\n",
        "            else:\n",
        "                self.inc_sample_count()\n",
        "                ret_dict = self._sequent_sample()\n",
        "        ret_dict = self.data_dict_to_tensor(data_dict=ret_dict,\n",
        "                                            data_types=self.data_types)\n",
        "        if self.preprocess:\n",
        "            ret_dict = self.preprocess_data_dict(data_dict=ret_dict,\n",
        "                                                 data_types=self.data_types,\n",
        "                                                 layout=self.layout,\n",
        "                                                 rescale=self.rescale_method)\n",
        "        if self.downsample_dict is not None:\n",
        "            ret_dict = self.downsample_data_dict(data_dict=ret_dict,\n",
        "                                                 data_types=self.data_types,\n",
        "                                                 factors_dict=self.downsample_dict,\n",
        "                                                 layout=self.layout)\n",
        "        return ret_dict\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data_dict = self._idx_sample(index=index)\n",
        "        return data_dict\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess_data_dict(data_dict, data_type=\"MIDDLE_EAST_VIS\", output_type=np.float32, layout='NHWT', rescale='ims'):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        data_dict:  Dict[str, Union[np.ndarray, torch.Tensor]]\n",
        "        data_types: Sequence[str]\n",
        "            The data types that we want to rescale. This mainly excludes \"mask\" from preprocessing.\n",
        "        layout: str\n",
        "            consists of batch_size 'N', seq_len 'T', channel 'C', height 'H', width 'W'\n",
        "        rescale:    str\n",
        "            'sevir': use the offsets and scale factors in original implementation.\n",
        "            '01': scale all values to range 0 to 1, currently only supports 'vil'\n",
        "        Returns\n",
        "        -------\n",
        "        data_dict:  Dict[str, Union[np.ndarray, torch.Tensor]]\n",
        "            preprocessed data\n",
        "        \"\"\"\n",
        "        if rescale == 'ims':\n",
        "            scale_dict = PREPROCESS_SCALE_IMS\n",
        "            offset_dict = PREPROCESS_OFFSET_IMS\n",
        "        else:\n",
        "            raise ValueError(f'Invalid rescale option: {rescale}.')\n",
        "        for i,data in enumerate(data_dict):\n",
        "            if isinstance(data, np.ndarray):\n",
        "                data = scale_dict[data_type] * (\n",
        "                        data.astype(output_type) +\n",
        "                        offset_dict[data_type])\n",
        "                data = change_layout_np(data=data,\n",
        "                                        in_layout='NHWT',\n",
        "                                        out_layout=layout)\n",
        "            elif isinstance(data, torch.Tensor):\n",
        "                data = scale_dict[data_type] * (\n",
        "                        data.float() +\n",
        "                        offset_dict[data_type])\n",
        "                data = change_layout_torch(data=data,\n",
        "                                            in_layout='NHWT',\n",
        "                                            out_layout=layout)\n",
        "            data_dict[i] = data\n",
        "        return data_dict\n",
        "\n",
        "    @staticmethod\n",
        "    def process_data_dict_back(data_dict, data_types=None, rescale='01'):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        data_dict\n",
        "            each data_dict[key] is a torch.Tensor.\n",
        "        rescale\n",
        "            str:\n",
        "                'sevir': data are scaled using the offsets and scale factors in original implementation.\n",
        "                '01': data are all scaled to range 0 to 1, currently only supports 'vil'\n",
        "        Returns\n",
        "        -------\n",
        "        data_dict\n",
        "            each data_dict[key] is the data processed back in torch.Tensor.\n",
        "        \"\"\"\n",
        "        if rescale == 'sevir':\n",
        "            scale_dict = PREPROCESS_SCALE_SEVIR\n",
        "            offset_dict = PREPROCESS_OFFSET_SEVIR\n",
        "        elif rescale == '01':\n",
        "            scale_dict = PREPROCESS_SCALE_01\n",
        "            offset_dict = PREPROCESS_OFFSET_01\n",
        "        else:\n",
        "            raise ValueError(f'Invalid rescale option: {rescale}.')\n",
        "        if data_types is None:\n",
        "            data_types = data_dict.keys()\n",
        "        for key in data_types:\n",
        "            data = data_dict[key]\n",
        "            data = data.float() / scale_dict[key] - offset_dict[key]\n",
        "            data_dict[key] = data\n",
        "        return data_dict\n",
        "\n",
        "    @staticmethod\n",
        "    def downsample_data_dict(data_dict, data_types=None, factors_dict=None, layout='NHWT'):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        data_dict:  Dict[str, Union[np.array, torch.Tensor]]\n",
        "        factors_dict:   Optional[Dict[str, Sequence[int]]]\n",
        "            each element `factors` is a Sequence of int, representing (t_factor, h_factor, w_factor)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        downsampled_data_dict:  Dict[str, torch.Tensor]\n",
        "            Modify on a deep copy of data_dict instead of directly modifying the original data_dict\n",
        "        \"\"\"\n",
        "        if factors_dict is None:\n",
        "            factors_dict = {}\n",
        "        if data_types is None:\n",
        "            data_types = data_dict.keys()\n",
        "        downsampled_data_dict = SEVIRDataLoader.data_dict_to_tensor(\n",
        "            data_dict=data_dict,\n",
        "            data_types=data_types)    # make a copy\n",
        "        for key, data in data_dict.items():\n",
        "            factors = factors_dict.get(key, None)\n",
        "            if factors is not None:\n",
        "                downsampled_data_dict[key] = change_layout_torch(\n",
        "                    data=downsampled_data_dict[key],\n",
        "                    in_layout=layout,\n",
        "                    out_layout='NTHW')\n",
        "                # downsample t dimension\n",
        "                t_slice = [slice(None, None), ] * 4\n",
        "                t_slice[1] = slice(None, None, factors[0])\n",
        "                downsampled_data_dict[key] = downsampled_data_dict[key][tuple(t_slice)]\n",
        "                # downsample spatial dimensions\n",
        "                downsampled_data_dict[key] = avg_pool2d(\n",
        "                    input=downsampled_data_dict[key],\n",
        "                    kernel_size=(factors[1], factors[2]))\n",
        "\n",
        "                downsampled_data_dict[key] = change_layout_torch(\n",
        "                    data=downsampled_data_dict[key],\n",
        "                    in_layout='NTHW',\n",
        "                    out_layout=layout)\n",
        "\n",
        "        return downsampled_data_dict\n",
        "\n",
        "    def _random_sample(self):\n",
        "        \"\"\"\n",
        "        Returns\n",
        "        -------\n",
        "        ret_dict\n",
        "            dict. ret_dict.keys() == self.data_types.\n",
        "            If self.preprocess == False:\n",
        "                ret_dict[imgt].shape == (batch_size, height, width, seq_len)\n",
        "        \"\"\"\n",
        "        num_sampled = 0\n",
        "        event_idx_list = nprand.randint(low=self.start_event_idx,\n",
        "                                        high=self.end_event_idx,\n",
        "                                        size=self.batch_size)\n",
        "        seq_idx_list = nprand.randint(low=0,\n",
        "                                      high=self.num_seq_per_event,\n",
        "                                      size=self.batch_size)\n",
        "        seq_slice_list = [slice(seq_idx * self.stride,\n",
        "                                seq_idx * self.stride + self.seq_len)\n",
        "                          for seq_idx in seq_idx_list]\n",
        "        ret_dict = {}\n",
        "        while num_sampled < self.batch_size:\n",
        "            event = self._load_event_batch(event_idx=event_idx_list[num_sampled],\n",
        "                                           event_batch_size=1)\n",
        "            for imgt_idx, imgt in enumerate(self.data_types):\n",
        "                sampled_seq = event[imgt_idx][[0, ], :, :, seq_slice_list[num_sampled]]  # keep the dim of batch_size for concatenation\n",
        "                if imgt in ret_dict:\n",
        "                    ret_dict[imgt] = np.concatenate((ret_dict[imgt], sampled_seq),\n",
        "                                                    axis=0)\n",
        "                else:\n",
        "                    ret_dict.update({imgt: sampled_seq})\n",
        "        return ret_dict\n",
        "\n",
        "    def _sequent_sample(self):\n",
        "        \"\"\"\n",
        "        Returns\n",
        "        -------\n",
        "        ret_dict:   Dict\n",
        "            `ret_dict.keys()` contains `self.data_types`.\n",
        "            `ret_dict[\"mask\"]` is a list of bool, indicating if the data entry is real or padded.\n",
        "            If self.preprocess == False:\n",
        "                ret_dict[imgt].shape == (batch_size, height, width, seq_len)\n",
        "        \"\"\"\n",
        "        assert not self.use_up, 'Data loader used up! Reset it to reuse.'\n",
        "        event_idx = self.curr_event_idx\n",
        "        seq_idx = self.curr_seq_idx\n",
        "        num_sampled = 0\n",
        "        sampled_idx_list = []   # list of (event_idx, seq_idx) records\n",
        "        while num_sampled < self.batch_size:\n",
        "            sampled_idx_list.append({'event_idx': event_idx,\n",
        "                                     'seq_idx': seq_idx})\n",
        "            seq_idx += 1\n",
        "            if seq_idx >= self.num_seq_per_event:\n",
        "                event_idx += 1\n",
        "                seq_idx = 0\n",
        "            num_sampled += 1\n",
        "\n",
        "        start_event_idx = sampled_idx_list[0]['event_idx']\n",
        "        event_batch_size = sampled_idx_list[-1]['event_idx'] - start_event_idx + 1\n",
        "\n",
        "        event_batch = self._load_event_batch(event_idx=start_event_idx,\n",
        "                                             event_batch_size=event_batch_size)\n",
        "        ret_dict = {\"mask\": []}\n",
        "        all_no_pad_flag = True\n",
        "        for sampled_idx in sampled_idx_list:\n",
        "            batch_slice = [sampled_idx['event_idx'] - start_event_idx, ]  # use [] to keepdim\n",
        "            seq_slice = slice(sampled_idx['seq_idx'] * self.stride,\n",
        "                              sampled_idx['seq_idx'] * self.stride + self.seq_len)\n",
        "            for imgt_idx, imgt in enumerate(self.data_types):\n",
        "                sampled_seq = event_batch[imgt_idx][batch_slice, :, :, seq_slice]\n",
        "                if imgt in ret_dict:\n",
        "                    ret_dict[imgt] = np.concatenate((ret_dict[imgt], sampled_seq),\n",
        "                                                    axis=0)\n",
        "                else:\n",
        "                    ret_dict.update({imgt: sampled_seq})\n",
        "            # add mask\n",
        "            no_pad_flag = sampled_idx['event_idx'] < self.end_event_idx\n",
        "            if not no_pad_flag:\n",
        "                all_no_pad_flag = False\n",
        "            ret_dict[\"mask\"].append(no_pad_flag)\n",
        "        if all_no_pad_flag:\n",
        "            # if there is no padded data items at all, set `ret_dict[\"mask\"] = None` for convenience.\n",
        "            ret_dict[\"mask\"] = None\n",
        "        # update current idx\n",
        "        self.set_curr_event_idx(event_idx)\n",
        "        self.set_curr_seq_idx(seq_idx)\n",
        "        return ret_dict\n",
        "\n",
        "    def _idx_sample(self, index):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        index\n",
        "            The index of the batch to sample.\n",
        "        Returns\n",
        "        -------\n",
        "        ret_dict\n",
        "            dict. ret_dict.keys() == self.data_types.\n",
        "            If self.preprocess == False:\n",
        "                ret_dict[imgt].shape == (batch_size, height, width, seq_len)\n",
        "        \"\"\"\n",
        "        event_idx = (index * self.batch_size) // self.num_seq_per_event\n",
        "        seq_idx = (index * self.batch_size) % self.num_seq_per_event\n",
        "        num_sampled = 0\n",
        "        sampled_idx_list = []  # list of (event_idx, seq_idx) records\n",
        "        while num_sampled < self.batch_size:\n",
        "            sampled_idx_list.append({'event_idx': event_idx,\n",
        "                                     'seq_idx': seq_idx})\n",
        "            seq_idx += 1\n",
        "            if seq_idx >= self.num_seq_per_event:\n",
        "                event_idx += 1\n",
        "                seq_idx = 0\n",
        "            num_sampled += 1\n",
        "\n",
        "        start_event_idx = sampled_idx_list[0]['event_idx']\n",
        "        event_batch_size = sampled_idx_list[-1]['event_idx'] - start_event_idx + 1\n",
        "\n",
        "        # event_batch.shape = (num_events, :, :, raw_seq_len)\n",
        "        event_batch = self._load_event_batch(event_idx=start_event_idx,\n",
        "                                             event_batch_size=event_batch_size)\n",
        "        ret = np.empty(0) # @ check what dimensions we want to initialize data with\n",
        "        for sampled_idx in sampled_idx_list:\n",
        "            batch_slice = [sampled_idx['event_idx'] - start_event_idx, ]  # use [] to keepdim\n",
        "            seq_slice = slice(sampled_idx['seq_idx'] * self.stride,\n",
        "                              sampled_idx['seq_idx'] * self.stride + self.seq_len)\n",
        "            \n",
        "            sampled_seq = event_batch[batch_slice, :, :, seq_slice] # @ check what is batch_slice - does not make sense yet\n",
        "            ret = np.concatenate((ret, sampled_seq), axis=0)\n",
        "\n",
        "        ret = torch.from_numpy(ret)    \n",
        "        \n",
        "        if self.preprocess:\n",
        "            ret_dict = self.preprocess_data_dict(data_dict=ret_dict,\n",
        "                                                 data_types=self.data_types,\n",
        "                                                 layout=self.layout,\n",
        "                                                 rescale=self.rescale_method)\n",
        "\n",
        "        if self.downsample_dict is not None:\n",
        "            ret_dict = self.downsample_data_dict(data_dict=ret_dict,\n",
        "                                                 data_types=self.data_types,\n",
        "                                                 factors_dict=self.downsample_dict,\n",
        "                                                 layout=self.layout)\n",
        "        return ret\n"
      ],
      "metadata": {
        "id": "MBIkl-T93aSW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}